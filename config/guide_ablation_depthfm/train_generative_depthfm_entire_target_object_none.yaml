base_config:
- config/logging.yaml
- config/wandb.yaml
- config/dataset/dataset_sam_train.yaml
- config/dataset/dataset_sam_val.yaml
- config/dataset/dataset_sam_vis.yaml

model:
  name: DepthFMAmodal
  kwargs:
    ckpt_path: work_dir/ckp/depthfm-v1.ckpt
    guide_type: 'none'

depth_normalization:
  type: sam_depth

augmentation:
  lr_flip_p: 0.5

dataloader:
  num_workers: 8
  effective_batch_size: 128
  max_train_batch_size: 8
  seed: 2024  # to ensure continuity when resuming from checkpoint

# Training settings
trainer:
  name: DepthFMAmodalTrainer
  training_noise_scheduler:
    pretrained_path: stable-diffusion-2
  init_seed: 2024  # use null to train w/o seeding
  save_period: 5000
  # save_period: 10
  backup_period: 5000
  validation_period: 2500
  visualization_period: 2500
  # validation_period: 20
  # visualization_period: 20
  max_grad_norm: 0.01
  w_occ: 0.7
  loss_stategy: entire_target_object

gt_depth_type: depth_gt
gt_mask_type: valid_mask_raw

max_epoch: 10000  # a large enough number
max_iter: 15000  # usually converges at around 20k

optimizer:
  name: Adam

loss:
  name: mse_loss
  kwargs:
    reduction: mean

lr: 3.0e-05
scale_lr: 1
lr_scheduler:
  name: IterExponential
  kwargs:
    total_iter: 12000
    final_ratio: 0.01
    warmup_steps: 100

# Validation (and visualization) settings
validation:
  denoising_steps: 50
  ensemble_size: 1  # simplified setting for on-training validation
  processing_res: 0
  match_input_res: false
  resample_method: bilinear
  main_val_metric: abs_relative_difference
  main_val_metric_goal: minimize
  init_seed: 2024

eval:
  alignment: least_square
  align_max_res: null
  eval_metrics:
  - abs_relative_difference
  - squared_relative_difference
  - rmse_linear
  - rmse_log
  - log10
  - delta1_acc
  - delta2_acc
  - delta3_acc
  - i_rmse
  - silog_rmse
